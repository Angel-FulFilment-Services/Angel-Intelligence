# ==============================================================================
# Angel Intelligence - Combined vLLM + Transcription (Thor/Jetson ARM64)
# ==============================================================================
# Single container with both vLLM (port 8000) and WhisperX transcription (port 8001)
# Uses dustynv/vllm which has working CUDA-enabled PyTorch
#
# Build: docker build -f Dockerfile.vllm-transcription -t angel-intelligence:vllm-transcription-arm64 .
# Run:   docker run --runtime nvidia -p 8000:8000 -p 8001:8001 angel-intelligence:vllm-transcription-arm64

FROM dustynv/vllm:0.9.2-r36.4-cu128-24.04

LABEL org.opencontainers.image.title="Angel Intelligence vLLM + Transcription (Thor/Jetson)"
LABEL org.opencontainers.image.description="Combined vLLM and WhisperX transcription service for ARM64"

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV ANGEL_ENV=production

WORKDIR /workspace

# Install additional system dependencies (ffmpeg for whisperx, supervisor for process management)
RUN apt-get update && apt-get install -y --no-install-recommends \
        ffmpeg \
        sox libsox-fmt-all \
        supervisor \
    && rm -rf /var/lib/apt/lists/*

# Install torchaudio from Jetson pip server (matches base PyTorch version)
RUN pip install --no-cache-dir torchaudio \
    --index-url https://pypi.jetson-ai-lab.io/jp6/cu128/+simple/ \
    --extra-index-url https://pypi.org/simple

# Install WhisperX and dependencies
RUN pip install --no-cache-dir faster-whisper
RUN pip install --no-cache-dir git+https://github.com/m-bain/whisperx.git --no-deps

# Install pyannote for speaker diarization
RUN pip install --no-cache-dir pyannote.audio || \
    pip install --no-cache-dir git+https://github.com/pyannote/pyannote-audio.git

# Install WhisperX deps and transcription dependencies
RUN pip install --no-cache-dir pandas nltk more-itertools

# Copy requirements and install remaining deps
COPY requirements/base.txt requirements/
COPY requirements/transcription.txt requirements/
RUN pip install --no-cache-dir -r requirements/transcription.txt 2>/dev/null || true

# Copy application code
COPY src/ src/

# Create supervisor config directory
RUN mkdir -p /etc/supervisor/conf.d /var/log/supervisor

# Create supervisor config to run both services
RUN cat > /etc/supervisor/supervisord.conf << 'EOF'
[supervisord]
nodaemon=true
logfile=/var/log/supervisor/supervisord.log
pidfile=/var/run/supervisord.pid
user=root

[program:vllm]
command=/bin/bash -c 'python3 -m vllm.entrypoints.openai.api_server --model "${VLLM_MODEL:-Qwen/Qwen2.5-72B-Instruct-AWQ}" --max-model-len "${MAX_MODEL_LEN:-8192}" --gpu-memory-utilization "${GPU_MEMORY_UTILIZATION:-0.85}" --tensor-parallel-size "${TENSOR_PARALLEL_SIZE:-1}" --max-num-seqs "${MAX_NUM_SEQS:-32}" --enable-lora --max-loras "${MAX_LORAS:-4}" --max-lora-rank "${MAX_LORA_RANK:-64}" --trust-remote-code --host 0.0.0.0 --port 8000'
autostart=true
autorestart=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr
stderr_logfile_maxbytes=0

[program:transcription]
command=python3 -m uvicorn src.api:app --host 0.0.0.0 --port 8001
autostart=true
autorestart=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr
stderr_logfile_maxbytes=0
EOF

# Expose both ports
EXPOSE 8000 8001

# Default: run both services via supervisor
CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/supervisord.conf"]
