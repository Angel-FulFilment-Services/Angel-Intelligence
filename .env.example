# ==============================================================================
# Angel Intelligence - Environment Configuration
# ==============================================================================
# Copy this file to .env and configure for your environment

# ==============================================================================
# ENVIRONMENT MODE
# ==============================================================================
# production = Full Kubernetes cluster on Jetson Nanos
# development = Local single-instance mode
ANGEL_ENV=development

# Unique worker identifier (used in logs and for job locking)
WORKER_ID=local-dev

# ==============================================================================
# API AUTHENTICATION
# ==============================================================================
# Bearer token for API authentication
# MUST be at least 64 characters for security
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(64))"
API_AUTH_TOKEN=

# ==============================================================================
# DATABASE CONFIGURATION (MySQL - 'ai' connection)
# ==============================================================================
AI_DB_HOST=localhost
AI_DB_PORT=3306
AI_DB_DATABASE=ai
AI_DB_USERNAME=root
AI_DB_PASSWORD=

# ==============================================================================
# R2 STORAGE CONFIGURATION (Cloudflare R2)
# ==============================================================================
R2_ENDPOINT=https://your-account-id.r2.cloudflarestorage.com
R2_ACCESS_KEY_ID=your-r2-access-key
R2_SECRET_ACCESS_KEY=your-r2-secret-key
R2_BUCKET=call-recordings

# ==============================================================================
# PBX RECORDING SOURCES
# ==============================================================================
# Live PBX recordings (tried first)
PBX_LIVE_URL=https://pbx.angelfs.co.uk/callrec/

# Archive PBX recordings (fallback)
PBX_ARCHIVE_URL=https://afs-pbx-callarchive.angelfs.co.uk/

# ==============================================================================
# LOCAL DEVELOPMENT STORAGE
# ==============================================================================
# Set to a folder path to use local files instead of downloading from PBX/R2
# Leave empty for production (will download from PBX sources)
# Example: LOCAL_STORAGE_PATH=C:\recordings
LOCAL_STORAGE_PATH=

# ==============================================================================
# MODEL CONFIGURATION
# ==============================================================================
# Base path for model storage (NFS mount in production)
MODELS_BASE_PATH=./models

# Whisper model for transcription
# Options: tiny, base, small, medium, large
# Smaller = faster but less accurate; larger = slower but more accurate
WHISPER_MODEL=medium

# Analysis model (fine-tunable for call analysis)
# HuggingFace model ID or local path
ANALYSIS_MODEL=Qwen/Qwen2.5-Omni-7B

# Analysis model local path (overrides ANALYSIS_MODEL if set)
ANALYSIS_MODEL_PATH=

# Analysis model quantization (int4, int8, or empty for none)
# Use for memory-constrained environments
ANALYSIS_MODEL_QUANTIZATION=

# Chat model (base model, NOT fine-tuned)
# Used for conversations, summaries, ad-hoc queries
CHAT_MODEL=Qwen/Qwen2.5-Omni-7B

# Chat model local path (overrides CHAT_MODEL if set)
CHAT_MODEL_PATH=

# Chat model quantization
CHAT_MODEL_QUANTIZATION=

# Analysis mode: 'audio' or 'transcript'
# audio = Direct audio analysis with Qwen2.5-Omni (better for tone/emotion)
# transcript = Text-only analysis (faster, works with any LLM)
ANALYSIS_MODE=audio

# ==============================================================================
# PROCESSING CONFIGURATION
# ==============================================================================
# Seconds between checking for new recordings
POLL_INTERVAL=30

# Maximum concurrent processing jobs per worker
MAX_CONCURRENT_JOBS=1

# Maximum retry attempts for failed jobs
MAX_RETRY_ATTEMPTS=3

# Hours to wait before retrying failed jobs
RETRY_DELAY_HOURS=1

# Transcription segmentation: 'word' or 'sentence'
# word = Word-level timestamps (for karaoke feature)
# sentence = Sentence-level timestamps
TRANSCRIPT_SEGMENTATION=word

# ==============================================================================
# PII REDACTION
# ==============================================================================
# Enable PII detection and redaction
ENABLE_PII_REDACTION=true

# ==============================================================================
# GPU CONFIGURATION
# ==============================================================================
# CUDA device IDs to use (comma-separated), or -1 for CPU only
CUDA_VISIBLE_DEVICES=0

# ==============================================================================
# MODEL HOT RELOAD (Production only)
# ==============================================================================
# Enable hot-swapping models without restart (for production clusters)
ENABLE_MODEL_HOT_RELOAD=false

# ==============================================================================
# MOCK MODE (Development only)
# ==============================================================================
# Use mock responses instead of real LLM inference
# Useful for testing without GPU
USE_MOCK_MODELS=false
