# ==============================================================================
# Angel Intelligence - Transcription Pod Dockerfile (Jetson ARM64 / Thor)
# ==============================================================================
# Uses NGC vLLM base which has working CUDA-enabled PyTorch on Thor
#
# Build: docker build -f Dockerfile.transcription-jetson -t angel-intelligence:transcription-arm64 .
# Run:   docker run --runtime nvidia -p 8001:8001 angel-intelligence:transcription-arm64

# Use NGC vLLM - same base as working vllm-arm64 image (has CUDA PyTorch)
FROM nvcr.io/nvidia/vllm:26.01-py3

LABEL org.opencontainers.image.title="Angel Intelligence Transcription (Thor/Jetson)"
LABEL org.opencontainers.image.description="Shared WhisperX transcription service for ARM64"

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV ANGEL_ENV=production

WORKDIR /workspace

# Install additional system dependencies (ffmpeg for whisperx audio loading)
RUN apt-get update && apt-get install -y --no-install-recommends \
        ffmpeg \
        sox libsox-fmt-all \
    && rm -rf /var/lib/apt/lists/*

# Verify base image has CUDA PyTorch
RUN python3 -c "import torch; print('Base PyTorch:', torch.__version__, 'CUDA:', torch.cuda.is_available())"

# Pin torch version to prevent overwrites, then install audio deps
# Use --no-deps on packages that might pull torch
RUN TORCH_VERSION=$(python3 -c "import torch; print(torch.__version__)") && \
    pip install --no-cache-dir --no-deps torchaudio && \
    pip install --no-cache-dir soundfile

# Verify CUDA still works after torchaudio
RUN python3 -c "import torch; assert torch.cuda.is_available(), 'CUDA broken after torchaudio!'; print('CUDA OK')"

# Copy requirements
COPY requirements/base.txt requirements/
COPY requirements/transcription.txt requirements/

# Install WhisperX dependencies (no-deps to avoid torch overwrite)
RUN pip install --no-cache-dir faster-whisper
RUN pip install --no-cache-dir git+https://github.com/m-bain/whisperx.git --no-deps

# Install pyannote with --no-deps, then install its deps manually (excluding torch)
RUN pip install --no-cache-dir --no-deps pyannote.audio pyannote.core pyannote.database pyannote.pipeline
RUN pip install --no-cache-dir asteroid-filterbanks einops hmmlearn lightning \
    omegaconf pytorch-metric-learning rich scikit-learn scipy \
    semver tensorboardX speechbrain 2>/dev/null || true

# Verify CUDA still works after all installs
RUN python3 -c "import torch; assert torch.cuda.is_available(), 'CUDA broken!'; print('Final CUDA check: OK')"

# Install minimal WhisperX deps and other transcription dependencies
RUN pip install --no-cache-dir pandas nltk more-itertools
RUN pip install --no-cache-dir --no-deps -r requirements/transcription.txt 2>/dev/null || true

# Copy only necessary code
COPY src/api/app.py src/api/
COPY src/api/auth.py src/api/
COPY src/api/routes.py src/api/
COPY src/api/routes_live.py src/api/
COPY src/api/__init__.py src/api/
COPY src/config/ src/config/
COPY src/database/ src/database/
COPY src/services/ src/services/
COPY src/__init__.py src/

EXPOSE 8001

# Run with multiple workers for concurrent request handling
# Thread pool handles GPU work, workers handle I/O parallelism
CMD ["python3", "-m", "uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8001", "--workers", "2"]
