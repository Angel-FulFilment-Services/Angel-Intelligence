# ==============================================================================
# Angel Intelligence - Worker Pod Requirements (Shared Services Mode)
# ==============================================================================
# Lightweight HTTP orchestrator - calls vLLM and Transcription services via HTTP
# NO local ML models - everything is proxied to shared GPU services
# Image size: ~300MB (minimal, no torch/transformers)
#
# Build: docker build -f Dockerfile.worker -t angel-intelligence:worker .
# Run:   python -m src.worker.worker
#
# Environment variables:
#   LLM_API_URL=http://vllm-server:8000/v1
#   TRANSCRIPTION_SERVICE_URL=http://transcription-service:8001

# Include base dependencies
-r base.txt

# ------------------------------------------------------------------------------
# HTTP Client (for calling shared services)
# ------------------------------------------------------------------------------
httpx==0.27.2

# ------------------------------------------------------------------------------
# Database
# ------------------------------------------------------------------------------
mysql-connector-python==9.1.0

# ------------------------------------------------------------------------------
# Cloud Storage (R2/S3 for audio files)
# ------------------------------------------------------------------------------
boto3==1.35.74

# ------------------------------------------------------------------------------
# Audio File Handling (reading audio for upload to transcription service)
# ------------------------------------------------------------------------------
soundfile==0.12.1

# ------------------------------------------------------------------------------
# PII Detection and Redaction (runs locally on worker)
# ------------------------------------------------------------------------------
presidio-analyzer==2.2.355
presidio-anonymizer==2.2.355
spacy==3.7.6
# Note: After install, run: python -m spacy download en_core_web_sm
# Using small model (en_core_web_sm) instead of large to reduce size

# ------------------------------------------------------------------------------
# JSON Repair (for handling malformed LLM responses)
# ------------------------------------------------------------------------------
json-repair==0.30.3
